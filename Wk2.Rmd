---
title: "Capstone"
author: "NJL"
date: "18/06/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is the first Assignment of the Data Science Capstone project.

The *goal* of this project is just to display that I've gotten used to working with the data and that I am on track to create my prediction algorithm. A report on R Pubs (http://rpubs.com/) that explains my exploratory analysis and my goals for the eventual app and algorithm is submitted as part of the assignment. This document should be concise and explain only the major features of the data I have identified and briefly summarize my plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. I should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to: 

1. Demonstrate that I've downloaded the data and have successfully loaded it in.

2. Create a basic report of summary statistics about the data sets.

3. Report any interesting findings that you amassed so far.

4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

### Loading Libraries

The libraries loaded are -
1. wordCloud - presentation of words proportional to the frequency
2. RColorBrewer - To set the color in the word cloud plot
3. text2vec - For calculating the word frequencies for various n-grams
4. kableExtra - To output result in table format

```{r lib, echo=FALSE, warning=FALSE, message=FALSE}
library("wordcloud")
library("RColorBrewer")
library("text2vec")
library("kableExtra")
```


### Loading data

This Assignment, I will be working on the data associated with the english language.
Three data files containing text are availabe in the folder 'en_US' which was downloaded from the link in the course material.
Due to low memory in my laptop, I chose *text2vec* package for text mining activities

```{r read, echo=FALSE, warning=FALSE, cache=TRUE}
news <- readLines("./en_US/en_US.news.txt", encoding = "UTF-8")
twitter <- readLines("./en_US/en_US.twitter.txt", encoding = "UTF-8")
blogs <- readLines("./en_US/en_US.blogs.txt", encoding = "UTF-8")
```

### Preparing list of Profane and Stop Words

Downloaded the base version of profane words list from Google and collected Stop Words from the Net.

```{r blockwords, echo=FALSE, warning=FALSE}
prof <- read.csv("Profane.csv", header = FALSE)
prof <- prof[1]
prof <- as.array(prof[[1]])
prof <- tolower(prof)
stop <- readLines("stopwords.txt", encoding = "UTF-8")
stop <- as.array(stop)
```

### Functions for Preprocessing

Two functions *cleanTxt* and *rmWords* are defined for preprocessing. The rmWords function removes the profane and stop words. The cleanTxt function removes standalone numbers, non text Ascii characters, whitespaces and converts the text to lowercase

```{r prepfns, echo=FALSE}
cleanTxt <- function(x)
{
  x1 <- gsub('[\x21-\x2F|\x3A-\x40|\x5B-\x60|\x7B-\x7E]', '', x) # Remove Punctuations
  x1 <- gsub('[^\x01-\x7E]', '', x1)                            # Remove non text Ascii chars
  x1 <- tolower(x1)
  x1 <- gsub("\\b\\d+\\b", "", x1)                                # Remove Numbers
  x1 <- gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", "", x1, perl=TRUE)     # Remove white spaces
  x1 <- rmWords(x1, prof)                                         # Remove Profane words
#  x1 <- rmWords(x1, stop)                                         # Remove Stop words
}

rmWords <- function(x, words)
  gsub(sprintf("(*UCP)\\b[a-zA-Z]*(%s)[a-zA-Z]*\\b",
               paste(sort(words, decreasing = TRUE), collapse = "|")),
       "", x, perl = TRUE)
```

### Function to compute frequency of words

Function itoken() and create_vocabulary() from text2vec is used to arrive at the word frequencies for 1,2,3,4 and 5-gram words

```{r token, echo=FALSE}
wrdFrq <- function(x)
{
  tok_func = word_tokenizer
  it_train = itoken(x, tokenizer = tok_func, progressbar = FALSE)
  vocab = create_vocabulary(it_train)
}
wrdFrq2 <- function(x)
{
  tok_func = word_tokenizer
  it_train = itoken(x, tokenizer = tok_func, progressbar = FALSE)
  vocab = create_vocabulary(it_train, ngram = c(2L, 2L))
}
wrdFrq3 <- function(x)
{
  tok_func = word_tokenizer
  it_train = itoken(x, tokenizer = tok_func, progressbar = FALSE)
  vocab = create_vocabulary(it_train, ngram = c(3L, 3L))
}
wrdFrq4 <- function(x)
{
  tok_func = word_tokenizer
  it_train = itoken(x, tokenizer = tok_func, progressbar = FALSE)
  vocab = create_vocabulary(it_train, ngram = c(4L, 4L))
}
wrdFrq5 <- function(x)
{
  tok_func = word_tokenizer
  it_train = itoken(x, tokenizer = tok_func, progressbar = FALSE)
  vocab = create_vocabulary(it_train, ngram = c(5L, 5L))
}
```

### Analysing the files

The size of the file, the number of words in each file and the number of sentences in each of the file is computed.

```{r files, echo=FALSE, cache=TRUE}
s1 <- file.info("./en_US/en_US.news.txt")$size
s2 <- file.info("./en_US/en_US.twitter.txt")$size
s3 <- file.info("./en_US/en_US.blogs.txt")$size

s1_lines <- length(news)
s2_lines <- length(twitter)
s3_lines <- length(blogs)

x <- wrdFrq(news)
s1_words <- sum(x$term_count)
x <- wrdFrq(twitter)
s2_words <- sum(x$term_count)
x <- wrdFrq(blogs)
s3_words <- sum(x$term_count)
```

### Data Info

The summary of the data in the 3 files are tabulated.

```{r table, echo=FALSE}
x <- data.frame(Source = c("News", "Twitter", "Blogs"), Size = c(s1/2^20, s2/2^20, s3/2^20), Word = c(s1_words, s2_words, s3_words), Sentence = c(s1_lines, s2_lines, s3_lines))
names(x) <- c("Source", "Size in MB", "Word Count", "Sentence Count")
x %>% kbl() %>% kable_styling()
```

### Sampling the Data

Since the size of the text files are too high for my computer's memory capacity, the files are sampled and all processing are done on the sampled data. 10% of the files are samples.

```{r sampling, cache=TRUE, echo=FALSE}
set.seed(4096)
blogs_s <- sample(blogs, length(blogs) * 0.01)
news_s <- sample(news, length(news) * 0.01)
twitter_s <- sample(twitter, length(twitter) * 0.01)

# deleting data that are no more required to release memory
rm(news)
rm(twitter)
rm(blogs)
```

### Cleaning sampled Data

Cleaning of sampled Data is done by calling the functions defined earlier
```{r clean, cache=TRUE, echo=FALSE}
blogs_sc <- cleanTxt(blogs_s)
news_sc <- cleanTxt(news_s)
twitter_sc <- cleanTxt(twitter_s)
```

### Computing Frequency of word occurances

The computing is done for each source (sampled) and then on the total sampled text for 1-gram
The stop words are filtered for 1-gram processing.

```{r blogs, echo=FALSE, cache=TRUE}
blogs_scs <- rmWords(blogs_sc, stop)                           # Remove Stop words
x <- wrdFrq(blogs_scs)
df <- as.data.frame(x$term)
df$count <- x$term_count
colnames(df)[1] <- "word"

df <- df[order(-df$count),]

set.seed(1234)
png(filename = "blogs.png")
wordcloud(words = df$word, freq = df$count, min.freq = 1, max.words=400, 
          random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
invisible(dev.off())

```

### Word Cloud representing Blogs Data
```{r plots1, echo=FALSE, fig.cap="", out.width = '100%'}
knitr::include_graphics("blogs.png")
```

### Word Cloud representing News Data
```{r news, echo=FALSE, cache=TRUE}
news_scs <- rmWords(news_sc, stop)
x <- wrdFrq(news_scs)
df <- as.data.frame(x$term)
df$count <- x$term_count
colnames(df)[1] <- "word"

df <- df[order(-df$count),]

set.seed(1234)
png(filename = "news.png")
wordcloud(words = df$word, freq = df$count, min.freq = 1, max.words=400, 
          random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
invisible(dev.off())

```
```{r plots2, echo=FALSE, fig.cap="", out.width = '100%'}
knitr::include_graphics("news.png")
```

### Word Cloud representing Twitter Data
```{r tweet, echo=FALSE, cache=TRUE}
twitter_scs <- rmWords(twitter_sc, stop)
x <- wrdFrq(twitter_scs)
df <- as.data.frame(x$term)
df$count <- x$term_count
colnames(df)[1] <- "word"

df <- df[order(-df$count),]

set.seed(1234)
png(filename = "twitter.png")
wordcloud(words = df$word, freq = df$count, min.freq = 1, max.words=400, 
          random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
invisible(dev.off())

```
```{r plots3, echo=FALSE, fig.cap="", out.width = '100%'}
knitr::include_graphics("twitter.png")
```

### Word Cloud representing combined Data
```{r all, echo=FALSE, cache=TRUE}

allText <- c(news_scs, blogs_scs, twitter_scs)
x <- wrdFrq(allText)
df <- as.data.frame(x$term)
df$count <- x$term_count
colnames(df)[1] <- "word"

df <- df[order(-df$count),]
write.csv(df, "Oneg.csv")

set.seed(1234)
png(filename = "allText.png")
wordcloud(words = df$word, freq = df$count, min.freq = 1, max.words=400, 
          random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
invisible(dev.off())
```
```{r plots4, echo=FALSE, fig.cap="", out.width = '100%'}
knitr::include_graphics("allText.png")
```

### Processing and output of 2,3,4,5-gram words

```{r 2g, echo=FALSE, cache=TRUE}
allText <- c(news_sc, blogs_sc, twitter_sc)
x <- wrdFrq2(allText)
df <- as.data.frame(x$term)
df$count <- x$term_count
colnames(df)[1] <- "word"

df <- df[order(-df$count),]
write.csv(df, "Twog.csv")
df <- df[1:30,]

par(mar=c(11,4,4,4))
barplot(height=df$count, 
        names.arg=df$word, col = "steelblue", 
        cex.names=0.8,
        las = 2,
        ylab = "Count",
        main = "Top 30 frequently used 2-gram words in Data"
        )
```



```{r 3g, echo=FALSE, cache=TRUE}
allText <- c(news_sc, blogs_sc, twitter_sc)
x <- wrdFrq3(allText)
df <- as.data.frame(x$term)
df$count <- x$term_count
colnames(df)[1] <- "word"

df <- df[order(-df$count),]
write.csv(df, "Threeg.csv")
df <- df[1:20,]

par(mar=c(15,4,4,4))
barplot(height=df$count, 
        names.arg=df$word, col = "steelblue", 
        cex.names=0.8,
        las = 2,
        ylab = "Count",
        main = "Top 20 frequently used 3-gram words in Data"
        )
```



```{r 4g, echo=FALSE, cache=TRUE}
allText <- c(news_sc, blogs_sc, twitter_sc)
x <- wrdFrq4(allText)
df <- as.data.frame(x$term)
df$count <- x$term_count
colnames(df)[1] <- "word"

df <- df[order(-df$count),]
write.csv(df, "Fourg.csv")
df <- df[1:20,]

par(mar=c(15,4,4,4))
barplot(height=df$count, 
        names.arg=df$word, col = "steelblue", 
        cex.names=0.8,
        las = 2,
        ylab = "Count",
        main = "Top 20 frequently used 4-gram words in Data"
        )

```



```{r 5g, echo=FALSE, cache=TRUE}
allText <- c(news_sc, blogs_sc, twitter_sc)
x <- wrdFrq5(allText)
df <- as.data.frame(x$term)
df$count <- x$term_count
colnames(df)[1] <- "word"

df <- df[order(-df$count),]
write.csv(df, "Fiveg.csv")
df <- df[1:20,]

par(mar=c(15,4,4,4))
barplot(height=df$count, 
        names.arg=df$word, col = "steelblue", 
        cex.names=0.8,
        las = 2,
        ylab = "Count",
        main = "Top 20 frequently used 5-gram words in Data"
        )
```

### Observations and Plans to go ahead

The Unigram output shows that the word "just" stands apart together with "get", "good", "love", ....
These words with high frequency can be used when no matches/prediction is found for the input text.

There are too many entries in the multi gram words. Need to filter with a cutoff frequency so that the search time is reduced.

Maybe 4 and 5-grams can be greatly reduced or removed, will try this out later.

On the Shiny side, need 2 data sets, one on the server side which will be a large one and one on the client side which is the actual data for prediction. The mirror of the client data will also be available at the server end.

The User entries are stored and when the storage reaches suffcient amount of data, modelling will be done again giving more weightage to the user input than the existing data in the server, the new model will then be updated to the client.

This is the plan now, depending on the performance and other alternatives, this plan can change.

### References:

Ascii Codes: https://theasciicode.com.ar/

Analysing Text using text2vec: https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html

Document Term Matrix : https://www.dustinstoltz.com/blog/2020/12/1/creating-document-term-matrix-comparison-in-r

Text mining and wordCloud : http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know



